# -*- coding: utf-8 -*-
"""Compétition ZINDI-INONDATION MALAWI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-2Av7HYHeQdEqgcWZJ9y5k0g85b2x6rS

**Comptetition description**<br>
Goal: Predict flood in Malawi using machine learning<br>
Data: Flood data in 2015 and 2019<br>
Evaluation metric: RMSE<br>
Timeline: 31 May 2020
"""

#!pip install geopandas
#!pip install plotly
#!pip install pandas-profiling[notebook,html]
#!pip install geopy
!pip install -U -q PyDrive

"""## Packages générals"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import warnings
import matplotlib.pyplot as plt
from geopy.geocoders import Nominatim
#import geopandas
import plotly.graph_objects as go
from pandas_profiling import ProfileReport
import matplotlib.pyplot as plt
# %matplotlib inline
warnings.filterwarnings(action='ignore')
plt.style.use('default')

"""## Importation de librairie pour la lecteur de fichier dans drive"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

from sklearn.metrics import mean_squared_error 
RMSE = lambda target, pred: np.sqrt(mean_squared_error(target, pred))

def get_csv_file_from_drive(link, filename):
  auth.authenticate_user()
  gauth = GoogleAuth()
  gauth.credentials = GoogleCredentials.get_application_default()
  drive = GoogleDrive(gauth)
  fluff, id = link.split('=')
  downloaded = drive.CreateFile({'id':id}) 
  downloaded.GetContentFile(filename)  
  file_ = pd.read_csv(filename)
  return file_

# DEFINING CONSTANT VARIABLES
PATH_TRAIN_FILE = "https://raw.githubusercontent.com/abdjiber/competiton-zindi-flood-prediction-malawi-data/master/Train.csv"
PATH_SUBMISSION = "https://raw.githubusercontent.com/abdjiber/competiton-zindi-flood-prediction-malawi-data/master/SampleSubmission.csv"

df = pd.read_csv(PATH_TRAIN_FILE).set_index('Square_ID')
df.rename({"X": 'Longitude', "Y":"Latitude"}, axis=1, inplace=True)

df_locations = get_csv_file_from_drive('https://drive.google.com/open?id=1fSoJCiab_bFPOblV79_djG1TLChjLcUo', 'locations.csv')

"""**Variables description**
* X: Longitude
* Y: Latitude
* Elevation: Mean elevation over the rectangle, based on [this](https://developers.google.com/earth-engine/datasets/catalog/USGS_SRTMGL1_003) dataset in Google Earth Engine.
* LC_Type1_mode: Most areas are predominantly grasslands, savannah or cropland.
* Weekly Precipitation. Historical rainfall data for each rectangle, for 18 weeks beginning 2 months before the flooding. Rainfall estimates from [this](https://developers.google.com/earth-engine/datasets/catalog/TRMM_3B42) dataset in Google Earth Engine.</br></br>

**Note**:
One degree on earth surface is approximately 111km, so 0.01 degrees is approximately 1.11km. Thus each square is approximately 1.11km x 1.11km sized. The X, Y co-ordiates represent the middle of the square. If you plot an X-Y chart with a dot for every position you'll end up with a map of southern Malawi.
"""

# SPLITTING TRAIN AND TEST DATA
common_cols_train_test = ['Longitude', 'Latitude', 'elevation', 'LC_Type1_mode']
target = df['target_2015'] # TARGET VARIABLE
df.drop('target_2015', axis=1, inplace=True)
precipitation_cols_2015 = list(df.columns[(df.columns.str.contains('2014') | df.columns.str.contains('2015'))])
precipitation_cols_2019 = list(df.columns[df.columns.str.contains('2019')])
train_cols = common_cols_train_test + precipitation_cols_2015
test_cols = common_cols_train_test + precipitation_cols_2019
df_train = df.loc[:, train_cols]
df_test = df.loc[:, test_cols]
# RENAMING PRECIPITATION COLUMNS
precipitation_cols_2015 = [x.replace('precip ', '') for x in precipitation_cols_2015]
precipitation_cols_2019 = [x.replace('precip ', '') for x in precipitation_cols_2019]
df_train.columns = common_cols_train_test + precipitation_cols_2015
df_test.columns = common_cols_train_test + precipitation_cols_2019

global locs 
locs = []
def get_locations_from_lat_lon(df, save=True, it = 1):
  df_locations = pd.DataFrame(['']*df.shape[0], columns=['locations full'])
  geolocator = Nominatim(user_agent="app", timeout=10)
  for i in range(df.shape[0]):
    location = geolocator.reverse(f"{df.Latitude[i]}, {df.Longitude[i]}")
    if location != None:
      adresse = location.address
      locs.append(adresse)
      df_locations.loc[i, 'locations full'] = adresse
      print(adresse)
    else:
      locs.append("")
      df_locations.loc[i, 'locations full'] = ""
    if save:
      df_locations.to_csv(f'data/locations{it}.csv', index=False, header=True)
  return df_locations

profile = ProfileReport(df=df_train)

def get_figure(height=10, width=10):
  fig = plt.figure(figsize=(height, width))
  ax = fig.add_subplot(1,1,1)
  return fig, ax

# VISUALISATION DES DONNEES AVEC GEOPANDAS
'''
gdf = geopandas.GeoDataFrame(
    df_train, geometry=geopandas.points_from_xy(df_train.X, df_train.Y))

world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))
fig, ax = get_figure(10, 8)
ax = world[world.continent == 'Africa'].plot(
    color='white', edgecolor='black', ax=ax)
gdf.plot(ax=ax, color='red')
'''

from IPython.display import HTML, display
display(HTML("<table><tr><td><img width=400 height=400 src='http://www.canalmonde.fr/r-annuaire-tourisme/monde/_cartes/malawi_2.jpg'></td><td><img width=400 height=400 src='https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Malawi_-_Southern.svg/800px-Malawi_-_Southern.svg.png'></td></tr></table>"))

layout = dict(
        geo = dict(
            showland = True,
            lataxis = dict(range=[-17,-14]),
            lonaxis = dict(range=[33,36])))

def choropleth(df, var, plot_target, target=[]):
  if plot_target:
    values = target
  else:
    values = df[var]
  X = df.X; Y = df.Y
  data = go.Scattergeo(
        lon = X,
        lat = Y,
        text = values ,
        mode = 'markers',
        marker_color = values,
        marker = dict(
            size = 8,
            opacity = 0.8,
            reversescale = True,
            autocolorscale = False,
            colorscale = 'hot',
            cmin = 0,
            color = values,
            cmax = values.max(),
            colorbar_title=var)
          )
  fig = go.Figure(data=data, layout=layout)
  fig.update_layout(
      width=800, height=800,
       geo_scope='africa'
     )
  return fig

def plot_vars(df_train, df_test):
  fig = plt.figure(figsize=(8, 40))
  #fig.suptitle('Précipitations Train & Test même périodes')
  X = df.Longitude; Y = df.Latitude;
  tmp1 = df_train.drop(['Longitude', 'Latitude'], axis=1)
  tmp2 = df_test.drop(['Longitude', 'Latitude'], axis=1)
  k = 1
  for colTrain, colTest in zip(list(tmp1), list(tmp2)):
    ax1 = fig.add_subplot(19, 2, k)
    ax2 = fig.add_subplot(19, 2, k + 1)
    ax1.scatter(X, Y, c=df_train[colTrain]); ax1.set_title(colTrain)
    ax2.scatter(X, Y, c=df_test[colTest]); ax2.set_title(colTest)
    k += 2
  fig.tight_layout()
  #plt.colorbar()

def plot_cols(df, c):
  plt.figure()
  plt.title(c.name)
  plt.scatter(df.Longitude, df.Latitude, c=c, cmap='hot')
  plt.colorbar()

plot_cols(df_train, df_train['elevation'])

plot_cols(df_train, df_train['LC_Type1_mode'])

plot_cols(df_train, target)

plot_vars(df_train.drop(['elevation', 'LC_Type1_mode'], axis=1), df_test.drop(['elevation', 'LC_Type1_mode'], axis=1))

tmp = df_train.copy()
tmp['target'] = target.values

fig, ax = get_figure(14,8)
sns.heatmap(tmp.corr(), annot=True,ax=ax)

sns.pairplot(tmp)

sns.pairplot(tmp.apply(np.log1p, axis=0))

list_districts_malawi_south = ["Balaka", "Blantyre", "Chikwawa", "Chiradzulu", "Machinga", "Mangochi", "Mulanje", "Mwanza", "Neno", "Nsanje", "Phalombe", "Thyolo", "Zomba"]
list_districts_malawi_center = ["Dedza", "Dowa", "Kasungu", "Lilongwe", "Mchinji", "Nkhotakota", "Ntcheu", "Ntchisi", "Salima"] 
list_districts_malawi_north = ["Chitipa", "Karonga", "Likoma", "Mzimba", "Nkhata Bay", "Rumphi"]
unique_locations = pd.Series(df_locations.locations.unique())
list_districts_mozambique = ["Tete", "Zambézia", "Niassa"]
list_all_districts = list_districts_malawi_south + list_districts_malawi_center + list_districts_malawi_north + list_districts_mozambique

def get_districts(df_locations):
  districts = []
  
  for i in range(df_train.shape[0]):
    split_adress = df_locations.locations[i].split(',')
    split_adress = [val.lstrip().rstrip() for val in split_adress] # AVOIDING BLANKS EX. ' Southern Region'
    k = 0
    for val in np.unique(split_adress): # TAKING ONLY UNIQUE VALUES OF THE SPLIT
      if val in list_all_districts:
        districts.append(val); k+=1
      elif "Chilwa" in val:
        districts.append("Zomba")
  return districts

#df_locations['locations full'] = df_locations['locations full'].apply(lambda x: x.replace("Nneno", "Neno"))
#df_locations['locations full'][[3126, 3256, 3257, 3386, 3387, 3647, 3781]] = "Pende 2, Chikwawa, Southern Region, Malawi"
#df_locations.rename({"locations full": "locations"}, axis=1, inplace=True)
#df_locations.to_csv('data/locations.csv', header=True, index=False)
#pd.Series(districts, name='disctricts').to_csv('disctricts.csv', index=False, header=True)

districts = get_districts(df_locations)
df_train['districts'] = districts
df_test['districts'] = districts
df_train['districts_encoded'] = df_train.districts.factorize()[0]
df_test['districts_encoded'] = df_test.districts.factorize()[0]

df_train = df_train.drop('districts', axis=1)
df_test = df_test.drop('districts', axis=1)

df_train = df_train.rename({col: f'S{i + 1}' for i, col in enumerate(precipitation_cols_2015)}, axis=1)
df_test = df_test.rename({col: f'S{i + 1}' for i, col in enumerate(precipitation_cols_2019)}, axis=1)

#ax = df_train.plot(y='Latitude', x='Longitude', kind='scatter', c=df_train.districts_encoded, cmap='Blues')

#!pip install catboost

np.random.seed(42)

#!pip install catboost

import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import  KMeans
from sklearn.svm import *
from sklearn.linear_model import *
from sklearn.ensemble import *
from sklearn.neighbors import *
#import catboost as catb

def basic_feature_eng_stats(df, precipitation_cols):
  df['max_precipitation'] = df[precipitation_cols].max(axis=1)
  df['min_precipitation'] = df[precipitation_cols].min(axis=1)
  df['mean_precipitation'] = df[precipitation_cols].mean(axis=1)
  df['std_precipitation'] = df[precipitation_cols].std(axis=1)
  return df

def basic_feature_eng_groupby(df, agg_col, cols):
  gb = df.groupby(agg_col).agg('mean')
  for col in cols:
    if col != agg_col:
      df[col + f'_mean_{agg_col}'] = df[agg_col].replace(gb[col].to_dict(), value=None)
    #df[col + 'gb'] = df[col].replace(gb, value=None)
  return df

def add_population_area_data(df):
  df['population'] = df.districts.replace(polutation_by_districts, value=None)
  df['area'] = df.districts.replace(area_by_districts, value=None)
  return df

def add_flood_data(df):
  for col, vals in dict_df_flood_data.items():
    df[col] = df.districts.replace(vals, value=None)
  return df

def encode_NS_Text(df):
  df.NS_Text = df.NS_Text.replace({"High":3, "Medium":2, "Low": 1}, value=None)
  return df

def count_encode(df, cols):
  for col in cols:
    counts = df[col].value_counts().to_dict()
    df[col + '_count'] = df[col].replace(counts, value=None)
  return 
  
def elbow(data):
  cost = []
  n_clusters = range(10, 100, 10)
  for n_cluster in n_clusters:
    min_ = 1e9
    for i in range(10):
      km = KMeans(n_clusters=n_cluster)
      km.fit(data)
      inertia = km.inertia_
      if inertia < min_:
        min_ = inertia
    cost.append(min_)
  plt.plot(n_clusters, cost)

target_crips = pd.Series(np.where(target <0.5, 0, 1))

def plot_tree(reg_model):
  fig, ax = get_figure(10, 8)
  return lgb.plot_importance(reg_model, ax=ax)

def scale_predictions(preds):
  preds = preds.where(preds < 1, 1)
  preds = preds.where(preds > 0, 0)
  return preds

def CV_clf(model, train, target, n_splits = 5, test=None):
    prob_oof = pd.Series(np.zeros((train.shape[0],)))
    pred_test = pd.Series(np.zeros((test.shape[0],)))
    scores_cv = pd.Series([0]*n_splits, dtype=np.float64)
    #params = {'classes_count':4, 'loss_function':'MultiClass'}
    KF = KFold(n_splits=n_splits)
    models = []
    for i, (idx_train, idx_test) in enumerate(KF.split(train, target)):
        print('Fold {}'.format(i))
        x_train, y_train = train.iloc[idx_train,:], target.iloc[idx_train]
        x_val, y_val = train.iloc[idx_test,:], target.iloc[idx_test]
        #model = clf(random_state=42)
        #model.fit(train[idx_train,:], target[idx_train])
        model.fit(x_train, y_train)
        #pred_train = model.predict_proba(x_val)[:, 1]#, 
        pred_train = model.predict(x_val)#, 
        prob_oof.iloc[idx_test] = pred_train
        if test is not None:
          pred_test += model.predict(test) / float(n_splits)
        score = RMSE(y_val, pred_train)
        models.append(model)
        print('RMSE {}'.format(score))
        scores_cv[i] = score
        if i == 0:
          x = x_val
          y = y_val
    print('Mean RMSE', scores_cv.mean())
    pred_train = scale_predictions(prob_oof)
    pred_test = scale_predictions(pred_test)
    return prob_oof, pred_test, scores_cv, models

def CV(train, target, n_splits = 5, test=None):
    prob_oof = pd.Series(np.zeros((train.shape[0],)))
    pred_test = pd.Series(np.zeros((test.shape[0],)))
    scores_cv = pd.Series([0]*n_splits, dtype=np.float64)
    #params = {'classes_count':4, 'loss_function':'MultiClass'}
    KF = KFold(n_splits=n_splits)
    models = []
    for i, (idx_train, idx_test) in enumerate(KF.split(train, target)):
        print('Fold {}'.format(i))
        x_train, y_train = train.iloc[idx_train,:], target.iloc[idx_train]
        x_val, y_val = train.iloc[idx_test,:], target.iloc[idx_test]
        #model = RandomForestClassifier()#probability=True
        #model = CatBoostClassifier(classes_count=4, loss_function='MultiClass')
        #model = CatBoostClassifier(learning_rate=0.01, n_estimators=1000, classes_count=4, loss_function='MultiClass', thread_count=10)
        model = lgb.LGBMRegressor(random_state=42)
        #model.fit(train[idx_train,:], target[idx_train])
        model.fit(x_train, y_train, eval_set=(x_val, y_val), verbose=0)
        #pred_train = model.predict_proba(x_val)[:, 1]#, 
        pred_train = model.predict(x_val)#, 
        prob_oof.iloc[idx_test] = pred_train
        if test is not None:
          pred_test += model.predict(test) / float(n_splits)
        score = RMSE(y_val, pred_train)
        models.append(model)
        print('RMSE {}'.format(score))
        scores_cv[i] = score
        if i == 0:
          x = x_val
          y = y_val
    print('Mean RMSE', scores_cv.mean())
    pred_train = scale_predictions(prob_oof)
    pred_test = scale_predictions(pred_test)
    return prob_oof, pred_test, scores_cv, models

def rotation(data):
  '''
  # most frequently used degrees are 30,45,60
  input: dataframe containing Latitude(x) and Longitude(y)
  '''
  rot_45_x = (0.707 * data.Latitude) + (0.707 * data.Longitude)
  rot_45_y = (0.707 * data.Longitude) + (0.707 * data.Latitude)
  rot_30_x = (0.866 * data.Latitude) + (0.5 * data.Longitude)
  rot_30_y = (0.866 * data.Longitude) + (0.5 * data.Latitude)
  data['rot_45_x'] = rot_45_x
  data['rot_45_y'] = rot_45_y
  data['rot_30_x'] = rot_30_x
  data['rot_30_y'] = rot_30_x
  return data

def submit(pred_test, filename=''):
  sub = pd.DataFrame()
  sub['Square_ID'] = df_train.index
  sub['target_2019'] = pred_test
  sub.target_2019 = sub.target_2019.where(sub.target_2019 > 0, 0)
  sub.to_csv(filename, index=False, header=True)

def kmeans(data, n_clusters):
  km = KMeans(n_clusters=n_clusters)
  km.fit(data)
  return km.predict(data), km.cluster_centers_

"""
#df_train_ = basic_feature_eng_groupby(df_train.copy(), 'districts_encoded', ['elevation'])
df_count_precip = count_encode(df_train.copy(), ['LC_Type1_mode', 'elevation', 'districts_encoded'])
df_train_rot = rotation(df_train.copy())
df_test_rot = rotation(df_test.copy())
clusters, clusters_centers = kmeans(df_train.loc[:, ['Longitude', 'Latitude']], 30)
#"Shire": [35.315278, -17.693333],
#"Nasenga": [35.21894, -14.53771],
southenr_rivers_lakes_coordinates = {  "Lisungwe": [34.74886, -15.58161],
                                     "Likangala": [35.6, -15.416667], "Naperi": [34.92553, -15.83283], "Mudi": [34.88184, -15.85844],
                                     "Ruo":[35.133333, -16.55], "Lake Malombe": [35.25, -14.666667], "Lake Chilwa": [35.7, -15.3]}
df_rivers = pd.DataFrame(southenr_rivers_lakes_coordinates.values(), columns=["Longitude", "Latitude"], index=southenr_rivers_lakes_coordinates.keys())
"""

def scale(train, test):
  scaler = MinMaxScaler()
  scaler.fit(train)
  train_scaled = pd.DataFrame(scaler.transform(train), columns=list(train))
  test_scaled = pd.DataFrame(scaler.transform(test), columns=list(test))
  return train_scaled, test_scaled

def compute_distance_from_lake_rivers(df, df_rivers):
  df_distance = pd.DataFrame(np.zeros((df.shape[0], df_rivers.shape[0])), columns=df_rivers.index, index=df.index)
  for i in range(df.shape[0]):
    for j, river in enumerate(df_rivers.index):
      df_distance.iloc[i, j] = haversine((df.Latitude[i], df.Longitude[i]), (df_rivers.Latitude[j], df_rivers.Longitude[j]))
  df_distance = df_distance.add_prefix('dist_from_')
  df_distance.to_csv('distances_from_rivers_lakes.csv', header=True, index=True)
  return df_distance

df_distance = compute_distance_from_lake_rivers(df_train, df_rivers)

"""
fig = plt.figure(figsize=(14, 16))
for i, col in enumerate(df_distance.columns):
  ax = fig.add_subplot(3, 3, i +1)
  df_train.plot(y='Latitude', x='Longitude', kind='scatter', c=df_distance[col], cmap='hot', ax=ax)
  ax.set_title(col)
"""

tmp_train = df_train.copy()
tmp_test = df_test.copy()
tmp_train['clusters'] = clusters
tmp_test['clusters'] = clusters

train_plus_dist = pd.concat([df_train, df_distance], axis=1)
test_plus_dist = pd.concat([df_test, df_distance], axis=1)

#from sklearn.decomposition import PCA
#pca = pd.DataFrame(MinMaxScaler().fit_transform(df_train[['Longitude', 'Latitude']]), columns=['PCA1', 'PCA2'], index=df_train.index)
#train_plus_dist = pd.concat([train_plus_dist,pca], axis=1)
#test_plus_dist = pd.concat([test_plus_dist,pca], axis=1)

#train_plus_dist = train_plus_dist.drop(['rot_30_x', 'rot_30_y', 'rot_45_x', 'rot_45_y'], axis=1)
#test_plus_dist = test_plus_dist.drop(['rot_30_x', 'rot_30_y', 'rot_45_x', 'rot_45_y'], axis=1)
#train_plus_dist = train_plus_dist.drop(['clusters'], axis=1)
#test_plus_dist = test_plus_dist.drop(['clusters'], axis=1)

#train = basic_feature_eng_groupby(train_plus_dist, 'districts_encoded', list(train_plus_dist))
#test = basic_feature_eng_groupby(test_plus_dist, 'districts_encoded', list(test_plus_dist))

#train_plus_dist['clusters'] = clusters
#test_plus_dist['clusters'] = clusters
#train.shape, test.shape

#train_plus_dist_rot = rotation(train_plus_dist)
#test_plus_dist_rot = rotation(test_plus_dist)

train_scaled, test_scaled = scale(train_plus_dist, test_plus_dist)

cols_preci_renamed = [f'S{i + 1}' for i in range(len(precipitation_cols_2015))]

prob_oof, pred_test, scores_cv, models = CV(train_scaled, target, test=test_scaled)

clf = RandomForestRegressor(random_state=42, n_estimators=400)#SVR(C=.01, kernel='poly')
prob_oof_rf, pred_test_rf, scores_cv, models = CV_clf(clf, train_scaled, target, test=test_scaled)

plot_preds(target, pred_test_rf)

def plot_preds(c_train, c_test):
  fig = plt.figure(figsize=(12,6))
  ax1 = fig.add_subplot(1,2, 1)
  ax2 = fig.add_subplot(1,2, 2)
  df_train.plot(y='Latitude', x='Longitude', kind='scatter', c=c_train, cmap='hot', ax=ax1)
  df_test.plot(y='Latitude', x='Longitude', kind='scatter', c=c_test, cmap='hot', ax=ax2)

fig = plt.figure(figsize=(12,6))
ax1 = fig.add_subplot(1,2, 1)
ax2 = fig.add_subplot(1,2, 2)
df_train.plot(y='Latitude', x='Longitude', kind='scatter', c=target, cmap='hot', ax=ax1)
df_test.plot(y='Latitude', x='Longitude', kind='scatter', c=pred_test.where(pred_test < 0.5, 1), cmap='hot', ax=ax2)

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

df_preci_train = df_train[cols_preci_renamed].reset_index(drop=True)
df_preci_test = df_test[cols_preci_renamed].reset_index(drop=True)
df_preci_train['is_test'] = 0
df_preci_test['is_test'] = 1
df_precip = pd.concat([df_preci_train, df_preci_test], axis=0)

X = df_precip.drop('is_test', axis=1)
y = df_precip.is_test
rf = RandomForestRegressor(random_state=42)
rf.fit(X, y)
prob_rf = rf.predict(X)
#idx_sort = np.argsort(f_imp)[::-1]
f_imp = rf.feature_importances_

pd.Series(f_imp, index=list(X)).sort_values().plot.bar()

pd.Series(prob_rf).unique()

fig = plt.figure(figsize=(12,6))
ax1 = fig.add_subplot(1,2, 1)
ax2 = fig.add_subplot(1,2, 2)
(prob_rf[:df_train.shape[0]]*df_train.S12).plot.hist(ax=ax1)
df_test.S12.plot.hist(ax=ax2)

submit(pred_test, 'sub_lb_20_03_11_19.csv')

lgb.plot_tree(models[0], figsize=(14,14))

target.apply(np.log1p).plot.hist()

ax = scale_predictions(prob_oof).plot()
scale_predictions(pred_test).plot(color='orange', ax=ax)

plot_tree(models[0])

#y.value_counts()
x.describe()

def model_by_col(train, test, target, col):
  train = train.reset_index(drop=True)
  target = target.reset_index(drop=True)
  unique_col = train[col].unique()
  prob_oof = pd.Series(np.zeros((train.shape[0],)))
  pred_test = pd.Series(np.zeros((test.shape[0],)))
  scores_cv = pd.Series([0]*unique_col, dtype=np.float64)
  for u in unique_col:
    idx = train.loc[(df[col] == u).values, :].index
    x, y = train.loc[idx, :], target[idx]
    model = lgb.LGBMRegressor(random_state=42)
    model.fit(x, y)
    preds = model.predict(x)
    pred_test[idx] = model.predict(test.loc[idx, :])
    prob_oof[idx] = preds
    score = RMSE(y, target[idx])
    print('Score', score)
  print('whool pred: ', RMSE(target, prob_oof))
  return pred_test

print(list(df_train))
print(list(df_test))

model_by_col(df_train.copy(), target, "LC_Type1_mode")

df_precip_15 = df_train.loc[:, precipitation_cols_2015]

diff_precip = df_precip_15.diff(axis=1).fillna(0).add_suffix('diff_precip_')
train_diff_precip = pd.concat([df_train, diff_precip], axis=1)

prob_oof, scores_cv, fold_4, clf_4 = CV(train_diff_precip, target)

#sns.catplot(data=df_train, col="LC_Type1_mode",kind="count", x="LC_Type1_mode")
tmp = df_train_.copy()
tmp['count_districts'] = df_train_.districts_encoded.replace(df_train_.districts_encoded.value_counts().to_dict(), value=None)

prob_oof, scores_cv, models = CV(tmp, target)

df_train['districts_encoded'] = df_train.districts.factorize()[0]

prob_oof, scores_cv, models = CV(df_train.drop('districts', axis=1), target)

plot_tree(models[4])

def fit_lbg(df_train, target, reg=True):
  if reg:
    reg_lgb = lgb.LGBMRegressor(random_state=12)
    reg_lgb.fit(df_train, target)
    print('RMSE', RMSE(target, reg_lgb.predict(df_train)))
  else:
    reg_lgb = lgb.LGBMClassifier(random_state=12)
    reg_lgb.fit(df_train, target)
    preds = reg_lgb.predict_proba(df_train)[:, 1]
    print('RMSE', RMSE(target, preds))
  return plot_tree(reg_lgb)



df_train.elevation.plot.hist()

fit_lbg(df_train, target)
# reg train: RMSE 0.08440325373187604

fit_lbg(df_train, target_crips, False)
#RMSE 0.10298349607341106

"""## Population and Area data
[Reference Malawi 2018](https://en.wikipedia.org/wiki/Districts_of_Malawi)</br>
[Reference Mozambique (District of Niassa 2017)](https://fr.wikipedia.org/wiki/Province_de_Niassa)</br>
[Reference Mozambique (District of Tete 2007)](https://fr.wikipedia.org/wiki/Tete)</br>
[Reference Mozambique (District of Zambézia 2006)](https://fr.wikipedia.org/wiki/Province_de_Zamb%C3%A9zie)</br>
"""

polutation_by_districts = {"Balaka":438379, "Blantyre":1251484 , "Chikwawa":564684, "Machinga":735438, "Mulanje":684107, "Mwanza":130949, "Neno":138291, "Niassa":1810794, "Nsanje":299168, "Ntcheu":659608, "Phalombe":429450, "Tete":152909, "Thyolo":721456, "Zambézia":3794509, "Zomba":851737, "Chiradzulu": 356875}
area_by_districts = {"Balaka":2142, "Blantyre":2025, "Chikwawa":4878, "Machinga":3582, "Mulanje":2005, "Mwanza":756, "Neno":1561, "Niassa":122827, "Nsanje":1945, "Ntcheu":3251, "Phalombe":1323 , "Tete":149.3, "Thyolo":1666, "Zambézia":103127, "Zomba":2405, "Chiradzulu":761}

"""## Flood Risk Data
[Reference Malawi](https://geonode.wfp.org/layers/geonode%3Amwi_nhr_naturalshocksrisk_geonode_20140612)</br>
[Reference Mozambique](https://geonode.wfp.org/layers/geonode%3Amoz_nhr_naturalshockhazard_geonode_20170623)
"""

df_flood_risk_malawi = get_csv_file_from_drive('https://drive.google.com/open?id=16u27g1-hfy1eiclwqlXZITWbLZebl_Uq', 'flood_risk.csv')

mask_districts_malawi = [True if val in np.unique(districts) else False for val in df_flood_risk_malawi.ADM2_NAME]
df_flood_risk_malawi= df_flood_risk_malawi.loc[mask_districts_malawi, ['ADM2_NAME','Dr_Class', 'FloodClass', 'NS_Risk', 'NS_Class', 'NS_Text']].set_index('ADM2_NAME')
df_flood_risk_malawi

df_flood_risk_mozambique = get_csv_file_from_drive('https://drive.google.com/open?id=1AVgMym3pssfNg0fMB6fsQkQ-frBVfwuh', 'flood_risk_mozambique.csv')

flood_moz = pd.DataFrame(columns=list(df_flood_risk_malawi))
for district in ['NIASSA', 'ZAMBEZIA', 'TETE']:
  flood_moz = pd.concat([flood_moz, df_flood_risk_mozambique.loc[df_flood_risk_mozambique.PROVINCIA == district, list(df_flood_risk_malawi)].mode()], axis=0)
flood_moz.index = ['Niassa', 'Zambézia', 'Tete']
flood_moz.index.name = 'ADM2_NAME'

flood_moz

# CAOMBINING FLOOD DATA IN MALAWI AND MOZAMBIQUE
df_flood_data = pd.concat([df_flood_risk_malawi, flood_moz], axis=0)
dict_df_flood_data = df_flood_data.to_dict()
df_flood_data

plt.plot(df_train.loc["4e3c3896-14ce-11ea-bce5-f49634744a41", precipitation_cols_2015])
plt.figure()
plt.plot(np.fft.fft(df_train.loc["4e3c3896-14ce-11ea-bce5-f49634744a41", precipitation_cols_2015]))



#df_train_ = basic_feature_eng(df_train.copy(), precipitation_cols_2015)
#df_test_ = basic_feature_eng(df_test.copy(), precipitation_cols_2019)

train = add_population_area_data(df_train.copy())
test = add_population_area_data(df_test.copy())

train = encode_NS_Text(add_flood_data(train))
test = encode_NS_Text(add_flood_data(test))

train.head()

fit_lbg(train.drop(['districts'], axis=1), target)
# FE DEFAULT DATA + POPULATION & AREA: RMSE 0.08351786306885338

train2 = basic_feature_eng_groupby(train.copy(), 'districts', precipitation_cols_2015 + ['elevation', 'LC_Type1_mode'])

df_train2.head()

fit_lbg(train2.drop(['districts'], axis=1), target)

fit_lbg(df_train2, target)